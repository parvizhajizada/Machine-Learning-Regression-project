---
title: "House Sales Price prediction with machine learning"
author:
  - Duy Tuan Doan
  - Parviz Hajizada
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

For those who lived through 2007-2008 may not forget the Financial Crisis, which is mentioned as one of most financial crisis after the Great Depression in 1930s.

The crisis started from subprime mortage market in the United States and then quickly affected the banking system worldwide. One of most basic problem that is contribute to the crisis is overestimate/ underestimate house values.

At that time before a crisis, machine learning is not widely used due to the cost of avalability and technology. Perhaps, if machine learning is used back then, consumer may have clear insight before signing the mortage contract. In this paper, we are building few models to predict the price of house in King County - Washington between May 2014 and May 2015.
![King County - Washington](/King County - Washington.JPG)

The dataset was extracted from Kaggle [link](https://www.kaggle.com/harlfoxem/housesalesprediction), it has 21 columns, along with 21613 observations.

```{r results='hide', message=FALSE, warning=FALSE, include=FALSE}
# load necessary packages 
library(readr)
library(dplyr)
library(tibble)
library(corrplot)
library(ggplot2)
library(AER)
library(lmtest)
library(nnet)
library(MASS)
library(caret)
library(verification)
library(e1071)
library(janitor)
library(class)
library(kernlab)
library(verification)
library(tidyverse)
library(gmodels)
library(vcd)
library(lubridate)
library(gridExtra)
library(devtools)
library(maps)
library(mapdata)
library(ggmap)
# Load data
house <- read_csv("kc_house_data.csv")

```

### Summary of the data
#### Checing for NA

```{r}
any(is.na(house)) 
```
There is no NA

#### Checing summary statistics
```{r}
summary(house) 
```
```{r}
str(house) 
```

### Feature engineering
```{r}
table(house$yr_renovated)
```

20699 house has never been renovated out of 21613 which is around 96 %. We could feature engineer a column such as time passed since renovation but we will not because most of the house have been renovated. So we will feature engineer a column which shows if the house has been renovated or not regardless of year and we drop the yr_renovated column afterwards
```{r}
house <- house %>% mutate(year = year(date),     
                          # year at which the house is sold
                          month = month(date),   
                          # month at which the house is sold
                          age = year - yr_built, 
                          # time interval between year build 
                          # and year sold (i.e. age of house)
                          renovated = factor(ifelse(yr_renovated != 0, 1,0))) # 1 if the house is renovated, 0 if it is not
```

#### Dropping the yr_renovated column:
```{r}
house$yr_renovated <- NULL 
```

#### Distribution of certain variable:
```{r}
house$year <- NULL 
```

There are only two years so it is not likely to give much information and for the sake of model simplicity and for not making out of sample prediction infeasible, we decide to forego the marginal information gain Month variable stays because it has seasonal variations and it does not make out of sample prediction infeasible unlike year variable

#### Dropping certain columns
```{r}
house$id <- NULL
house$date <- NULL
```

id variable gives no information and date column enables us to see how price changes over time but once the year and month columns were created, we do not need the date column itself.

latitude and longitude variables are good for making heat map of price over a map which is done below but it is not so intiutive to expect the price being dependent on those variables explicitly. For that reason, we find the zipcode variable as a better location factor affecting the price and use it in estimation instead of latitude,longitude.

```{r}
table(house$zipcode)
```
kind of uniformaly distributed or at least not inbalanced. Not likely to overfit.

```{r}
table(house$bedrooms)
```
0 bedroom does not make much sense, hard to say if it is wrong data entry or there really are houses with 0 bedrooms. but we can safely drop these values because there are only 13 of them. Number of houses with 7 and more bedrooms are quite little we will aggregate them under 7+ bedrooms level.
we drop the house with 33 bedrooms because it is an outlier.
```{r}
house <- house %>% filter(bedrooms !=0, bedrooms !=33) 
# drop observations with 0 or 33 bedrooms
```
values above 7 will be coded as 7
```{r}
house$bedrooms[house$bedrooms > 7] <- 7
house$bedrooms <- factor(house$bedrooms)
```
####  Changing last level from 7 to 6+
```{r}
levels(house$bedrooms)
levels(house$bedrooms)[7] <- "6+"
```
####  Do the similar data processing to bathrooms column also
```{r}
table(house$bathrooms)
```
drop observations with rare number of bathrooms
```{r}
house <- house %>% filter(bathrooms !=0, bathrooms !=0.5,
                          bathrooms !=0.75, bathrooms !=1.25) 

```
####  values above 3.75 will be coded as 3.75
```{r}
house$bathrooms[house$bathrooms > 3.75] <- 3.75
house$bathrooms <- factor(house$bathrooms)
```
####  Changing last level from 3.75 to 3.5+
```{r}
levels(house$bathrooms)
levels(house$bathrooms)[11] <- "3.5+"
```
#### Do the similar data processing to floors column also
drop observations with rare number of floors
```{r}
table(house$floors)
house <- house %>% filter(floors !=3.5) 

```
#### values above 2.5 will be coded as 2.5
```{r}
house$floors[house$floors > 2.5] <- 2.5
house$floors <- factor(house$floors)
```
#### Changing last level from 2.5 to 2+
```{r}
levels(house$floors)
levels(house$floors)[4] <- "2+"
```
#### Do the similar data processing to grade column also
```{r}
table(house$grade)
```
#### values above 12 will be coded as 12
```{r}
house$grade[house$grade > 12] <- 12
house$grade <- factor(house$grade)
```
#### Changing last level from 12 to 12+
```{r}
levels(house$grade)
levels(house$grade)[9] <- "11+"
```
#### Do the similar data processing to condition column also values above 12 will be coded as 12
```{r}
table(house$condition)

house$condition[house$condition < 2] <- 2
house$condition <- factor(house$condition)
```
#### Changing last level from 12 to 12+
```{r}
levels(house$condition)
levels(house$condition)[1] <- "3-"
```
#### Drop observations with age being equal to -1 (Data entry mistake)
```{r}
table(house$age)
house <- house %>% filter(age !=-1)
```
#### Drop yr_built column because it is unnecassary once we have age column
```{r}
cor(house$yr_built, house$age) 
```
validation of claim by calculating correlation -0.999 correlation or by the fact that age is linear transformation of yr_built.
```{r}
house$yr_built <- NULL
```
#### Changing class of certain variables 
```{r}
vars_factor <- c("waterfront", "view", "zipcode", "month")
house[vars_factor] <- lapply(house[vars_factor], factor)
```
#### Drop highly correlated columns
```{r}
M <- cor(house[,c("price", "sqft_living", "sqft_lot", 
                  "sqft_basement", "age", 
                  "sqft_above", "sqft_living15", "sqft_lot15")])
corrplot.mixed(M) 
```
Drop sqft_above, sqft_living15, sqft_lot15 because of high correlation

### Data Visualization
```{r echo=FALSE}
p1 <- ggplot(train, aes(x=sqft_living, y=price)) + 
  geom_point()+
  geom_smooth()

p2 <- ggplot(train, aes(x=sqft_lot, y=price)) + 
  geom_point()+
  geom_smooth()

p3 <- ggplot(train, aes(x=sqft_basement, y=price)) + 
  geom_point()+
  geom_smooth()
```
```{r echo=FALSE}
grid.arrange(p1, p2, p3, ncol = 3)

p4 <- ggplot(data=train, aes(x=bedrooms, y=price)) +
  geom_bar(stat="identity")

p5 <- ggplot(data=train, aes(x=bathrooms, y=price)) +
  geom_bar(stat="identity")

p6 <- ggplot(data=train, aes(x=floors, y=price)) +
  geom_bar(stat="identity")

p7 <- ggplot(data=train, aes(x=waterfront, y=price)) +
  geom_bar(stat="identity")

p8 <- ggplot(data=train, aes(x=view, y=price)) +
  geom_bar(stat="identity")

p9 <- ggplot(data=train, aes(x=condition, y=price)) +
  geom_bar(stat="identity")

p10 <- ggplot(data=train, aes(x=grade, y=price)) +
  geom_bar(stat="identity")

p11 <- ggplot(data=train, aes(x=month, y=price)) +
  geom_bar(stat="identity")

p12 <- ggplot(data=train, aes(x=renovated, y=price)) +
  geom_bar(stat="identity")

grid.arrange(p4, p5, p6 ,p7 ,p8,
             p9 ,p10 ,p11, p12,
             nrow = 3, ncol=3)
```
### Map
```{r echo=FALSE}
states <- map_data("state")
ca_df <- subset(states, region == "washington")
counties <- map_data("county")
ca_county <- subset(counties, region == "washington")
king_county <- subset(ca_county, subregion == "king")
ca_base <- ggplot(data = king_county, mapping = aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = "black", fill = "gray")
ca_base
```
```{r echo=FALSE}
b <- ggplot(house, aes(x = long, y = lat))+
  geom_point(aes(color = price)) +
  scale_color_gradientn(colours = rainbow(5)) +
  theme(legend.position = "right")
b
```
### More visualization
```{r echo=FALSE}
ggplot(train, aes(x=sqft_living, y=price, color = bathrooms)) + 
  geom_point()
```
```{r echo=FALSE}
avg_price_ren <- house %>% group_by(renovated) %>% summarize(average_price = mean(price))
ggplot(avg_price_ren, aes(x = renovated, y = average_price, fill = renovated)) + 
  geom_bar(stat = "identity") +
  ggtitle("Mean price VS renovated")
```
```{r echo=FALSE}
avg_price_view <- house %>% group_by(view) %>% summarize(average_price = mean(price))
ggplot(avg_price_view, aes(x = view, y = average_price, fill = view)) + 
  geom_bar(stat = "identity") +
  ggtitle("Mean price VS view")
```

# Modelling
## Data Partitioning
```{r}
set.seed(1)
which_train <- createDataPartition(house$price, 
                                   p = 0.8, 
                                   list = FALSE) 
```
## Split data into training and test set 
```{r}
train <- house[which_train,]
test <- house[-which_train,]

```

## Linear Regression
```{r}
set.seed(1)

formula <- price ~ bedrooms + bathrooms + sqft_living +
  sqft_lot + floors + waterfront + view + condition + 
  grade + sqft_basement + zipcode +
  month + renovated

linear.train <- 
  train(formula, 
        data = train, 
        method = "lm")
```
### Summary
```{r}
summary(linear.train)
```
### Fitted values
```{r}
linear.train_fitted <- predict(linear.train, train)
```
### Predicted values
```{r}
linear.train_forecasts <- predict(linear.train, test)
```
### RMSE, R^2, MAE of test set
```{r}
postResample(pred = linear.train_forecasts, obs = test$price)
```
####         RMSE     Rsquared          MAE 
#### 1.488733e+05 8.296537e-01 8.981330e+04


### RMSE, R^2, MAE of train set
```{r}
postResample(pred = linear.train_fitted, obs = train$price)
```
####         RMSE     Rsquared          MAE 
#### 1.496397e+05 8.355731e-01 8.779856e+04

## SVM
### Setting train control
```{r}
ctrl_cv2 <- trainControl(method = "cv",
                         number = 2)
```
### Parameters of svmLinear
```{r}
modelLookup("svmLinear")
```
### Grid search
```{r}
parametersC <- data.frame(C = c(0.01, 0.1, 0.2, 0.5, 1, 5))
```
### Train data
```{r}
set.seed(1)
svm_Linear <- train(formula, 
                    data = train, 
                    method = "svmLinear",
                    tuneGrid = parametersC,
                    trControl = ctrl_cv2)
```
### Summary
```{r}
summary(svm_Linear)
```
### Fitted values
```{r}
svm_linear.train_fitted <- predict(svm_Linear, train)
```
### Predicted values
```{r}
svm_linear.train_forecasts <- predict(svm_Linear, test)
```
### RMSE, R^2, MAE of test set
```{r}
postResample(pred = svm_linear.train_forecasts, obs = test$price)
```
####         RMSE     Rsquared          MAE 
#### 1.513252e+05 8.279697e-01 8.291942e+04 

#### RMSE, R^2, MAE of train set
```{r}
postResample(pred = svm_linear.train_fitted, obs = train$price)
```
####         RMSE     Rsquared          MAE 
#### 1.579167e+05 8.290153e-01 8.222068e+04


### Parameters of svmRadial
```{r}
modelLookup("svmRadial")
```
### Grid Search
```{r}
parametersC_sigma <- 
  expand.grid(C = c(0.01, 0.05, 0.1, 0.5, 1, 5),
              sigma = c(0.05, 0.1, 0.2, 0.5, 1))
```              
### Train data
```{r}
set.seed(1)
svm_radial <- train(formula, 
                    data = train, 
                    method = "svmRadial",
                    tuneGrid = parametersC_sigma,
                    trControl = ctrl_cv2)
```
### Summary
```{r}
summary(svm_radial)
```
### Fitted values
```{r}
svm_radial.train_fitted <- predict(svm_radial, train)
```
### Predicted values
```{r}
svm_radial.train_forecasts <- predict(svm_radial, test)
```
### RMSE, R^2, MAE of test set
```{r}
postResample(pred = svm_radial.train_forecasts, obs = test$price)
```
####         RMSE     Rsquared          MAE 
#### 2.246180e+05 6.091055e-01 1.568402e+05

### RMSE, R^2, MAE of train set
```{r}
postResample(pred = treebag.train_fitted, obs = train$price)
```
####         RMSE     Rsquared          MAE 
#### 2.263118e+05 6.291737e-01 1.559959e+05 

## Parameters of Treebag
```{r}
modelLookup("treebag")
```
### Train data
```{r}
set.seed(1)
treebag.train <- train(formula, 
                       data = train, 
                       method = "treebag")
```
### Summary
```{r}
summary(treebag.train)
```
### Fitted values
```{r}
treebag.train_fitted <- predict(treebag.train, train)
```
### Predicted values
```{r}
treebag.train_forecasts <- predict(treebag.train, test)
```
### RMSE, R^2, MAE of test set
```{r}
postResample(pred = treebag.train_forecasts, obs = test$price)
```
####         RMSE     Rsquared          MAE 
#### 2.246180e+05 6.091055e-01 1.568402e+05

### RMSE, R^2, MAE of train set
```{r}
postResample(pred = treebag.train_fitted, obs = train$price)
```
####         RMSE     Rsquared          MAE 
#### 2.263118e+05 6.291737e-01 1.559959e+05 

## Bayesglm 
```{r}
modelLookup("bayesglm")
```
###  Train data
```{r}
set.seed(1)
bayesglm.train <- train(formula, 
                        data = train, 
                        method = "bayesglm")
```
### Summary
```{r}
summary(bayesglm.train)
```
### Fitted values
```{r}
bayesglm.train_fitted <- predict(bayesglm.train, train)
```
### Predicted values
```{r}
bayesglm.train_forecasts <- predict(bayesglm.train, test)
```
### RMSE, R^2, MAE of test set
```{r}
postResample(pred = bayesglm.train_forecasts, obs = test$price)
```
####         RMSE     Rsquared          MAE 
#### 1.488737e+05 8.296501e-01 8.981299e+04 


# RMSE, R^2, MAE of train set
```{r}
postResample(pred = bayesglm.train_fitted, obs = train$price)
```
####         RMSE     Rsquared          MAE 
#### 1.496397e+05 8.355730e-01 8.779858e+04 

## CART
```{r}
modelLookup("rpart1SE")
```
### Train data
```{r}
set.seed(1)
rpart1SE.train <- train(formula, 
                        data = train, 
                        method = "rpart1SE")
```
### Summary
```{r}
summary(rpart1SE.train)
```
### Fitted values
```{r}
rpart1SE.train_fitted <- predict(rpart1SE.train, train)
```
### Predicted values
```{r}
rpart1SE.train_forecasts <- predict(rpart1SE.train, test)
```
### RMSE, R^2, MAE of test set
```{r}
postResample(pred = rpart1SE.train_forecasts, obs = test$price)
```

####         RMSE     Rsquared          MAE 
#### 2.430796e+05 5.481711e-01 1.650317e+05


### RMSE, R^2, MAE of train set
```{r}
postResample(pred = rpart1SE.train_fitted, obs = train$price)
```
####         RMSE     Rsquared          MAE 
#### 2.434084e+05 5.649388e-01 1.644436e+05

# Conclusion
We have built 6 models to predict house price, and their result of test set are following:
<table width =100%>
 <tr>
  <th>Model</th>
  <th>RMSE</th>
  <th>Rsquared</th>
  <th>MAE</th>
 </tr>
 <tr>
  <td>Linear Regression</td>
  <td>1.488733e+05</td>
  <td>8.296537e-01</td>
  <td>8.981330e+04</td>
 </tr>
  <tr>
  <td>svmLinear</td>
  <td>1.513252e+05</td>
  <td>8.279697e-01</td>
  <td>8.291942e+04</td>
 </tr>
  <tr>
  <td>svmRadial</td>
  <td>2.634224e+05  </td>
  <td>4.849898e-01</td>
  <td>1.342765e+05</td>
 </tr>
  <tr>
  <td>Treebag</td>
  <td>2.246180e+05</td>
  <td>6.091055e-01</td>
  <td>1.568402e+05</td>
 </tr>
  <tr>
  <td>Bayesglm</td>
  <td>1.496397e+05 </td>
  <td> 8.355730e-01 </td>
  <td>8.779858e+04</td>
 </tr>
 <tr>
  <td>CART</td>
  <td>2.430796e+05</td>
  <td>5.481711e-01</td>
  <td>1.650317e+05</td>
 </tr>
</table>
```{r results="asis", echo=FALSE}
cat("
<style>
th {
  text-align: left;
}
td {
  height: 50px;
  vertical-align: bottom;
}
th, td {
  border-bottom: 1px solid #ddd;
}
tr:hover {background-color: #f5f5f5;}
</style>
")
```
Using Rsquared as criteria to choose the best models and RMSE is condition for choosing the best one we have *Bayesglm* model performing the best.

After finishing this project, we can be confident to replace the dataset with another dataset from other place and forecast the price of house. Although we never know exactly when the next finacial crisis, we hope our model can somehow provide support to consumers when they enter mortage market.
